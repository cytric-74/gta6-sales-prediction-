{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpek/F9gW4R8bdVuk1bLh/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cytric-74/gta6-sales-prediction-/blob/main/gta6_sales_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsZ1zjAk1_nF",
        "outputId": "9c389525-657a-4ece-ab4a-ac48155c092b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.4.26)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m122.9/126.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from googleapiclient.discovery import build"
      ],
      "metadata": {
        "id": "K9ocHsHmv8mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmUoz4WCmF0x"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Twitter data collection***"
      ],
      "metadata": {
        "id": "xgBFOp6KmdLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOKKEN_TWIITER = \"we need a token here\""
      ],
      "metadata": {
        "id": "pcDsVumynH7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_twitter_data(query, max_results=50):\n",
        "    headers = {\"Authorization\": f\"Bearer {TOKKEN_TWIITER}\"}\n",
        "    url = f\"https://api.twitter.com/2/tweets/search/recent?query={query}&tweet.fields=created_at,public_metrics&max_results={max_results}\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    tweets = response.json()('data', [])\n",
        "\n",
        "    df = pd.DataFrame([{\n",
        "        'text': tweet['text'],\n",
        "        'created_at': tweet['created_at'],\n",
        "        'likes': tweet['public_metrics']['like_count']\n",
        "    } for tweet in tweets])\n",
        "\n",
        "    df.to_csv('data/twitter_data.csv', index=False)\n",
        "    return df"
      ],
      "metadata": {
        "id": "vlY8M4DpnLgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Youtube data collection***"
      ],
      "metadata": {
        "id": "tRddiaLi70Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Y8VKdur879AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = 'need to get a youtube api key asap'\n",
        "youtube = build('youtube', 'v3', developerKey=API_KEY)"
      ],
      "metadata": {
        "id": "bWbuE0vL8ETM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_youtube_comments(query):\n",
        "    search_response = youtube.search().list(\n",
        "        q=query, part='snippet', type='video', maxResults=5\n",
        "    ).execute()\n",
        "\n",
        "    video_ids = [item['id']['videoId'] for item in search_response['items']]\n",
        "    all_comments = []\n",
        "\n",
        "    for vid in video_ids:\n",
        "        comment_response = youtube.commentThreads().list(\n",
        "            part='snippet', videoId=vid, textFormat='plainText', maxResults=50\n",
        "        ).execute()\n",
        "\n",
        "        for item in comment_response.get(\"items\", []):\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            all_comments.append(comment)\n",
        "\n",
        "    df = pd.DataFrame({'comment': all_comments})\n",
        "    df.to_csv('data/youtube_data.csv', index=False)\n",
        "    return df"
      ],
      "metadata": {
        "id": "F7tI7k898HcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Web Scrapping***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9PYVNNRV1Hj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_news(query):\n",
        "    formatted_query = query.replace(\" \", \"%20\")\n",
        "    search_url = f\"https://news.google.com/search?q={formatted_query}\"\n",
        "    response = requests.get(search_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    articles = []\n",
        "    for a in soup.select('article h3'):\n",
        "        if a.a:\n",
        "            title = a.text.strip()\n",
        "            link = \"https://news.google.com\" + a.a['href'][1:]\n",
        "            articles.append({'title': title, 'link': link})\n",
        "\n",
        "    df = pd.DataFrame(articles)\n",
        "    df.to_csv('data/news_articles.csv', index=False)\n",
        "    return df"
      ],
      "metadata": {
        "id": "luonrV2N8Xm7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}