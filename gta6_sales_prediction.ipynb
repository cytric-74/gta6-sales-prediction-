{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSJdnurtmBEny/mTlg+K82",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cytric-74/gta6-sales-prediction-/blob/main/gta6_sales_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsZ1zjAk1_nF",
        "outputId": "33a46f19-1e19-4c84-d3b5-88985918c6f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.4.26)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m122.9/126.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from googleapiclient.discovery import build\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "K9ocHsHmv8mE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZmUoz4WCmF0x"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Twitter data collection***"
      ],
      "metadata": {
        "id": "xgBFOp6KmdLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOKKEN_TWIITER = \"we need a token here\""
      ],
      "metadata": {
        "id": "pcDsVumynH7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_twitter_data(query, max_results=50):\n",
        "    headers = {\"Authorization\": f\"Bearer {TOKKEN_TWIITER}\"}\n",
        "    url = f\"https://api.twitter.com/2/tweets/search/recent?query={query}&tweet.fields=created_at,public_metrics&max_results={max_results}\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    tweets = response.json()('data', [])\n",
        "\n",
        "    df = pd.DataFrame([{\n",
        "        'text': tweet['text'],\n",
        "        'created_at': tweet['created_at'],\n",
        "        'likes': tweet['public_metrics']['like_count']\n",
        "    } for tweet in tweets])\n",
        "\n",
        "    df.to_csv('data/twitter_data.csv', index=False)\n",
        "    return df"
      ],
      "metadata": {
        "id": "vlY8M4DpnLgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Youtube data collection***"
      ],
      "metadata": {
        "id": "tRddiaLi70Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Y8VKdur879AE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = 'need to get a youtube api key asap'\n",
        "youtube = build('youtube', 'v3', developerKey=API_KEY)"
      ],
      "metadata": {
        "id": "bWbuE0vL8ETM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_youtube_comments(query):\n",
        "    search_response = youtube.search().list(\n",
        "        q=query, part='snippet', type='video', maxResults=5\n",
        "    ).execute()\n",
        "\n",
        "    video_ids = [item['id']['videoId'] for item in search_response['items']]\n",
        "    all_comments = []\n",
        "\n",
        "    for vid in video_ids:\n",
        "        comment_response = youtube.commentThreads().list(\n",
        "            part='snippet', videoId=vid, textFormat='plainText', maxResults=50\n",
        "        ).execute()\n",
        "\n",
        "        for item in comment_response.get(\"items\", []):\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            all_comments.append(comment)\n",
        "\n",
        "    df = pd.DataFrame({'comment': all_comments})\n",
        "    df.to_csv('data/youtube_data.csv', index=False)\n",
        "    return df"
      ],
      "metadata": {
        "id": "F7tI7k898HcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Web Scrapping***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9PYVNNRV1Hj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_news(query):\n",
        "    formatted_query = query.replace(\" \", \"%20\")\n",
        "    search_url = f\"https://news.google.com/search?q={formatted_query}\"\n",
        "    response = requests.get(search_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    articles = []\n",
        "    for a in soup.select('article h3'):\n",
        "        if a.a:\n",
        "            title = a.text.strip()\n",
        "            link = \"https://news.google.com\" + a.a['href'][1:]\n",
        "            articles.append({'title': title, 'link': link})\n",
        "\n",
        "    df = pd.DataFrame(articles)\n",
        "    df.to_csv('data/news_articles.csv', index=False)\n",
        "    return df"
      ],
      "metadata": {
        "id": "luonrV2N8Xm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Sentiment Analysis***"
      ],
      "metadata": {
        "id": "gLnJBPEMICkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(df, column):\n",
        "    df['sentiment'] = df[column].astype(str).apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
        "    return df"
      ],
      "metadata": {
        "id": "59pw21oMIH6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Model Prediction***"
      ],
      "metadata": {
        "id": "AVOqHxl5IMy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_and_predict():\n",
        "    twitter_df = analyze_sentiment(pd.read_csv(\"data/twitter_data.csv\"), \"text\")\n",
        "    youtube_df = analyze_sentiment(pd.read_csv(\"data/youtube_data.csv\"), \"comment\")\n",
        "    news_df = analyze_sentiment(pd.read_csv(\"data/news_articles.csv\"), \"title\")\n",
        "\n",
        "    avg_sentiments = pd.DataFrame({\n",
        "        'twitter_sentiment': [twitter_df['sentiment'].mean()],\n",
        "        'youtube_sentiment': [youtube_df['sentiment'].mean()],\n",
        "        'news_sentiment': [news_df['sentiment'].mean()]\n",
        "    })\n",
        "\n",
        "    # Dummy training data with similar structure (for demonstration)\n",
        "    X_train = pd.DataFrame({\n",
        "        'twitter_sentiment': np.random.uniform(-1, 1, 100),\n",
        "        'youtube_sentiment': np.random.uniform(-1, 1, 100),\n",
        "        'news_sentiment': np.random.uniform(-1, 1, 100),\n",
        "    })\n",
        "    y_train = X_train.mean(axis=1) * 50 + 85  # Just a mocked-up sales range\n",
        "\n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "        'MLP Regressor': MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)\n",
        "    }\n",
        "\n",
        "    predictions = {}\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        pred = model.predict(avg_sentiments)[0]\n",
        "        predictions[name] = round(pred, 2)\n",
        "        print(f\"{name} Predicted GTA 6 Sales (in millions):\", round(pred, 2))\n",
        "\n",
        "    result_df = avg_sentiments.copy()\n",
        "    for name, pred in predictions.items():\n",
        "        result_df[name.replace(\" \", \"_\").lower() + '_prediction'] = pred\n",
        "\n",
        "    result_df.to_csv(\"data/sales_prediction.csv\", index=False)"
      ],
      "metadata": {
        "id": "_PTHIFPUIajt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Running***"
      ],
      "metadata": {
        "id": "JWNl_nYhIfrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Collecting Twitter data...\")\n",
        "    fetch_twitter_data(\"GTA 6\")\n",
        "\n",
        "    print(\"Collecting YouTube comments...\")\n",
        "    fetch_youtube_comments(\"GTA 6 trailer\")\n",
        "\n",
        "    print(\"Scraping news articles...\")\n",
        "    scrape_news(\"GTA 6\")\n",
        "\n",
        "    print(\"Analyzing sentiment and predicting sales with multiple models...\")\n",
        "    combine_and_predict()\n",
        "    print(\"All done! Check the 'data/' folder for CSVs.\")"
      ],
      "metadata": {
        "id": "p7pwflvGKER0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}